{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e163282",
   "metadata": {},
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d80419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.stats import pearsonr\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec013f",
   "metadata": {},
   "source": [
    "## Load all stock data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== LOADING NEWS AND ALL STOCK DATA ===\")\n",
    "\n",
    "# Load news data with flexible date parsing (same approach as Task 1)\n",
    "news_df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
    "\n",
    "# Convert date column to string first for consistent handling\n",
    "news_df['date_str'] = news_df['date'].astype(str)\n",
    "\n",
    "# Extract basic date information using string operations\n",
    "news_df['year'] = news_df['date_str'].str.extract(r'(\\d{4})-\\d{2}-\\d{2}').astype(float)\n",
    "news_df['month'] = news_df['date_str'].str.extract(r'\\d{4}-(\\d{2})-\\d{2}').astype(float)\n",
    "news_df['date_simple'] = news_df['date_str'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# Extract time if available\n",
    "def extract_hour(date_str):\n",
    "    \"\"\"Extract hour from date string\"\"\"\n",
    "    try:\n",
    "        time_match = re.search(r'(\\d{1,2}):(\\d{2}):(\\d{2})', str(date_str))\n",
    "        if time_match:\n",
    "            return int(time_match.group(1))\n",
    "    except:\n",
    "        pass\n",
    "    return 0  # Default to midnight if no time found\n",
    "\n",
    "news_df['hour'] = news_df['date_str'].apply(extract_hour)\n",
    "\n",
    "print(f\"News Data: {len(news_df):,} articles\")\n",
    "print(f\"   Date extraction: {news_df['date_simple'].notna().sum():,} articles with dates\")\n",
    "\n",
    "# Load ALL 6 stock datasets\n",
    "stock_files = {\n",
    "    'AAPL': '../data/AAPL.csv',\n",
    "    'MSFT': '../data/MSFT.csv', \n",
    "    'GOOG': '../data/GOOG.csv',\n",
    "    'AMZN': '../data/AMZN.csv',\n",
    "    'NVDA': '../data/NVDA.csv',\n",
    "    'META': '../data/META.csv'\n",
    "}\n",
    "\n",
    "stocks_data = {}\n",
    "for stock_name, file_path in stock_files.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])  # Stock dates are clean\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df['stock'] = stock_name\n",
    "    stocks_data[stock_name] = df\n",
    "    print(f\" {stock_name}: {len(df):,} trading days\")\n",
    "\n",
    "# Combine all stocks for analysis\n",
    "all_stocks_df = pd.concat(stocks_data.values())\n",
    "all_stocks_df['date_only'] = all_stocks_df.index.date\n",
    "\n",
    "print(f\"\\n Date Ranges:\")\n",
    "print(f\"   News: {news_df['date_simple'].min()} to {news_df['date_simple'].max()}\")\n",
    "print(f\"   Stocks: {all_stocks_df.index.min()} to {all_stocks_df.index.max()}\")\n",
    "print(f\"   Total Stock Records: {len(all_stocks_df):,}\")\n",
    "\n",
    "# Preview data\n",
    "print(f\"\\n News Sample:\")\n",
    "print(news_df[['headline', 'date_simple', 'hour', 'publisher', 'stock']].head(3))\n",
    "print(f\"\\n Stocks Sample:\")\n",
    "print(all_stocks_df[['stock', 'Close', 'Volume']].head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aa4c5",
   "metadata": {},
   "source": [
    "## Date alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATE ALIGNMENT ===\")\n",
    "\n",
    "# Simple alignment: Use date_simple to match with stock dates\n",
    "# Since we have mixed date formats, we'll use the extracted date_simple\n",
    "\n",
    "def simple_date_alignment(news_row, stock_dates_dict):\n",
    "    \"\"\"\n",
    "    Simple alignment using extracted date_simple\n",
    "    \"\"\"\n",
    "    date_simple = news_row['date_simple']\n",
    "    stock_symbol = news_row['stock']\n",
    "    \n",
    "    if pd.isna(date_simple):\n",
    "        return None\n",
    "    \n",
    "    # Get trading dates for this stock\n",
    "    stock_dates = stock_dates_dict.get(stock_symbol, [])\n",
    "    \n",
    "    # Convert date_simple to datetime for comparison\n",
    "    try:\n",
    "        news_date = pd.to_datetime(date_simple).date()\n",
    "        \n",
    "        # Check if this date exists in stock data\n",
    "        if news_date in stock_dates:\n",
    "            return news_date\n",
    "        \n",
    "        # If not, find the closest trading day\n",
    "        future_dates = [d for d in stock_dates if d >= news_date]\n",
    "        if future_dates:\n",
    "            return min(future_dates)\n",
    "        \n",
    "        past_dates = [d for d in stock_dates if d <= news_date]\n",
    "        if past_dates:\n",
    "            return max(past_dates)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Get trading dates for each stock (as date objects)\n",
    "trading_dates_by_stock = {}\n",
    "for stock_name, df in stocks_data.items():\n",
    "    trading_dates_by_stock[stock_name] = sorted(df.index.date)\n",
    "\n",
    "print(\" Aligning news with trading days...\")\n",
    "\n",
    "# Apply simple alignment\n",
    "news_df['aligned_date'] = news_df.apply(\n",
    "    lambda row: simple_date_alignment(row, trading_dates_by_stock),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check alignment success\n",
    "aligned_count = news_df['aligned_date'].notna().sum()\n",
    "print(f\" Successfully aligned: {aligned_count:,}/{len(news_df):,} articles ({aligned_count/len(news_df)*100:.1f}%)\")\n",
    "\n",
    "# Show alignment by stock\n",
    "print(f\"\\n Alignment by Stock:\")\n",
    "for stock in stock_files.keys():\n",
    "    stock_news = news_df[news_df['stock'] == stock]\n",
    "    aligned_stock = stock_news['aligned_date'].notna().sum()\n",
    "    total_stock = len(stock_news)\n",
    "    if total_stock > 0:\n",
    "        print(f\"   {stock}: {aligned_stock:,}/{total_stock:,} articles ({aligned_stock/total_stock*100:.1f}%)\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\n Alignment Examples:\")\n",
    "aligned_samples = news_df[news_df['aligned_date'].notna()].head(5)\n",
    "for _, row in aligned_samples.iterrows():\n",
    "    print(f\"   {row['date_simple']} (hour: {row['hour']:02d}) ‚Üí {row['aligned_date']} | {row['stock']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f241f5",
   "metadata": {},
   "source": [
    "## Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SENTIMENT ANALYSIS FOR ALL STOCKS ===\")\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "try:\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    vader_available = True\n",
    "    print(\" Using VADER for sentiment analysis\")\n",
    "except:\n",
    "    vader_available = False\n",
    "    print(\"  VADER not available, using TextBlob only\")\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"Use VADER or fallback to TextBlob\"\"\"\n",
    "    if vader_available:\n",
    "        scores = vader_analyzer.polarity_scores(str(text))\n",
    "        return scores['compound']\n",
    "    else:\n",
    "        # Fallback to TextBlob\n",
    "        analysis = TextBlob(str(text))\n",
    "        return analysis.sentiment.polarity\n",
    "\n",
    "print(\"Analyzing sentiment for aligned articles...\")\n",
    "\n",
    "# Apply sentiment analysis to aligned news only\n",
    "aligned_news = news_df[news_df['aligned_date'].notna()].copy()\n",
    "\n",
    "# Primary sentiment method\n",
    "aligned_news['sentiment'] = aligned_news['headline'].apply(analyze_sentiment_vader)\n",
    "\n",
    "print(f\" Sentiment analysis completed on {len(aligned_news):,} aligned articles\")\n",
    "\n",
    "# Sentiment distribution by stock\n",
    "print(f\"\\n Sentiment Distribution by Stock:\")\n",
    "for stock in stock_files.keys():\n",
    "    stock_sentiments = aligned_news[aligned_news['stock'] == stock]['sentiment']\n",
    "    if len(stock_sentiments) > 0:\n",
    "        print(f\"   {stock}: {len(stock_sentiments):>5,} articles | Mean: {stock_sentiments.mean():.3f} | Range: {stock_sentiments.min():.3f} to {stock_sentiments.max():.3f}\")\n",
    "\n",
    "# Overall sentiment stats\n",
    "print(f\"\\n Overall Sentiment Statistics:\")\n",
    "print(f\"   Mean Sentiment: {aligned_news['sentiment'].mean():.3f}\")\n",
    "print(f\"   Positive Articles (>0.05): {len(aligned_news[aligned_news['sentiment'] > 0.05]):,} ({len(aligned_news[aligned_news['sentiment'] > 0.05])/len(aligned_news)*100:.1f}%)\")\n",
    "print(f\"   Negative Articles (<-0.05): {len(aligned_news[aligned_news['sentiment'] < -0.05]):,} ({len(aligned_news[aligned_news['sentiment'] < -0.05])/len(aligned_news)*100:.1f}%)\")\n",
    "print(f\"   Neutral Articles: {len(aligned_news[(aligned_news['sentiment'] >= -0.05) & (aligned_news['sentiment'] <= 0.05)]):,} ({len(aligned_news[(aligned_news['sentiment'] >= -0.05) & (aligned_news['sentiment'] <= 0.05)])/len(aligned_news)*100:.1f}%)\")\n",
    "\n",
    "# Show sentiment examples\n",
    "print(f\"\\n Sentiment Examples:\")\n",
    "sample_sentiments = aligned_news[['headline', 'stock', 'sentiment']].head(8)\n",
    "for _, row in sample_sentiments.iterrows():\n",
    "    sentiment_label = \"POSITIVE\" if row['sentiment'] > 0.05 else \"NEGATIVE\" if row['sentiment'] < -0.05 else \"NEUTRAL\"\n",
    "    print(f\"   {sentiment_label:>8} ({row['sentiment']:6.3f}) | {row['stock']} | {row['headline'][:55]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72412c",
   "metadata": {},
   "source": [
    "## Daily aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DAILY AGGREGATION: SENTIMENT & RETURNS ===\")\n",
    "\n",
    "# Calculate daily stock returns for all stocks\n",
    "print(\" Calculating daily returns for all stocks...\")\n",
    "for stock_name, df in stocks_data.items():\n",
    "    df['daily_return'] = df['Close'].pct_change() * 100  # Percentage returns\n",
    "\n",
    "# Aggregate daily sentiment by stock and date\n",
    "print(\" Aggregating daily sentiment scores...\")\n",
    "daily_sentiment = aligned_news.groupby(['stock', 'aligned_date']).agg({\n",
    "    'sentiment': ['mean', 'count'],  # Average sentiment and article count\n",
    "    'headline': 'count'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "daily_sentiment.columns = ['sentiment_mean', 'sentiment_count', 'article_count']\n",
    "daily_sentiment = daily_sentiment.reset_index()\n",
    "\n",
    "print(f\" Created {len(daily_sentiment):,} daily sentiment records\")\n",
    "print(f\"Date range: {daily_sentiment['aligned_date'].min()} to {daily_sentiment['aligned_date'].max()}\")\n",
    "\n",
    "# Show daily aggregation examples\n",
    "print(f\"\\n Daily Aggregation Examples:\")\n",
    "for stock in list(stock_files.keys())[:2]:  # Show first 2 stocks\n",
    "    stock_daily = daily_sentiment[daily_sentiment['stock'] == stock].head(3)\n",
    "    print(f\"\\n   {stock}:\")\n",
    "    for _, row in stock_daily.iterrows():\n",
    "        print(f\"      {row['aligned_date']}: {row['article_count']} articles, sentiment: {row['sentiment_mean']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9839b",
   "metadata": {},
   "source": [
    "## Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f5975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Merge sentiment with stock returns\n",
    "correlation_results = []\n",
    "\n",
    "for stock_name in stock_files.keys():\n",
    "    print(f\"\\nüîç Analyzing {stock_name}...\")\n",
    "    \n",
    "    # Get stock returns\n",
    "    stock_returns = stocks_data[stock_name][['daily_return']].copy()\n",
    "    stock_returns = stock_returns.reset_index()\n",
    "    stock_returns['date_only'] = stock_returns['Date'].dt.date\n",
    "    \n",
    "    # Get sentiment for this stock\n",
    "    stock_sentiment = daily_sentiment[daily_sentiment['stock'] == stock_name].copy()\n",
    "    \n",
    "    if len(stock_sentiment) > 0:\n",
    "        # Merge on date\n",
    "        merged_data = pd.merge(stock_sentiment, stock_returns, \n",
    "                              left_on='aligned_date', right_on='date_only')\n",
    "        \n",
    "        if len(merged_data) > 10:  # Need sufficient data points\n",
    "            # Calculate correlation\n",
    "            correlation, p_value = pearsonr(merged_data['sentiment_mean'], \n",
    "                                          merged_data['daily_return'])\n",
    "            \n",
    "            correlation_results.append({\n",
    "                'stock': stock_name,\n",
    "                'correlation': correlation,\n",
    "                'p_value': p_value,\n",
    "                'data_points': len(merged_data),\n",
    "                'mean_sentiment': merged_data['sentiment_mean'].mean(),\n",
    "                'mean_return': merged_data['daily_return'].mean()\n",
    "            })\n",
    "            \n",
    "            print(f\"    Correlation: {correlation:.4f} (p-value: {p_value:.4f})\")\n",
    "            print(f\"    Data points: {len(merged_data)} days\")\n",
    "            print(f\"    Mean return: {merged_data['daily_return'].mean():.3f}%\")\n",
    "            print(f\"    Mean sentiment: {merged_data['sentiment_mean'].mean():.3f}\")\n",
    "        else:\n",
    "            print(f\"    Insufficient data: {len(merged_data)} points\")\n",
    "    else:\n",
    "        print(f\"     No sentiment data for {stock_name}\")\n",
    "\n",
    "# Create correlation results dataframe\n",
    "if correlation_results:\n",
    "    corr_df = pd.DataFrame(correlation_results)\n",
    "    print(f\"\\nüèÜ CORRELATION RANKING:\")\n",
    "    corr_df = corr_df.sort_values('correlation', ascending=False)\n",
    "    \n",
    "    for _, row in corr_df.iterrows():\n",
    "        significance = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"\"\n",
    "        print(f\"   {row['stock']}: {row['correlation']:.4f} {significance} (p={row['p_value']:.4f})\")\n",
    "else:\n",
    "    print(\"‚ùå No correlation results calculated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
